{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Q2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e86a1b7e1d7a45ecb133555f6d52e3e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fe9eaa1498554c679046521aa762f61a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_18b7158a999e4e278965c469e8a13688",
              "IPY_MODEL_75add3ab3c38442baf0418175590c427"
            ]
          }
        },
        "fe9eaa1498554c679046521aa762f61a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "18b7158a999e4e278965c469e8a13688": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_71ce85335a9343d6881837dbd70fc440",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a26a88adfadb43d49c5b99f45db477f7"
          }
        },
        "75add3ab3c38442baf0418175590c427": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_31bcff9bc75c42e893b0299a32c27bb8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 51459105.01it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8ef1462d5d743b2be8658ee53951630"
          }
        },
        "71ce85335a9343d6881837dbd70fc440": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a26a88adfadb43d49c5b99f45db477f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31bcff9bc75c42e893b0299a32c27bb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8ef1462d5d743b2be8658ee53951630": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZichangYe/NBA-Analytics/blob/master/Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JccQsl2OtAOi"
      },
      "source": [
        "### Imports "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-12T21:23:02.741734Z",
          "start_time": "2020-10-12T21:23:02.010707Z"
        },
        "id": "5elHgLPSoiT7",
        "outputId": "ad1a8be0-cea1-4aff-888a-ec2e75d5e5e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch \n",
        "import torchvision\n",
        "import torch.nn as nn \n",
        "from IPython.display import Image \n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed = 12345\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7efdf290b5a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8Aj_LrAhwRP"
      },
      "source": [
        "#### Mount your google drive so you can save model checkpoints, and report your test results on the final best model after hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-10-12T21:23:31.609516Z",
          "start_time": "2020-10-12T21:23:31.596532Z"
        },
        "id": "utx0ySGqQbha",
        "outputId": "863ee44c-e580-43d1-cf22-1af3ec3807b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqzOqBsvs9kA"
      },
      "source": [
        "### Data loading "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-3MVXNOhdCc"
      },
      "source": [
        "##### Run the following cells to load the dataset. Setting download=True will download it for you."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rb7Ng7_VcwL",
        "outputId": "082e1ac6-06af-4f7a-acb5-eb0c36943847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120,
          "referenced_widgets": [
            "e86a1b7e1d7a45ecb133555f6d52e3e7",
            "fe9eaa1498554c679046521aa762f61a",
            "18b7158a999e4e278965c469e8a13688",
            "75add3ab3c38442baf0418175590c427",
            "71ce85335a9343d6881837dbd70fc440",
            "a26a88adfadb43d49c5b99f45db477f7",
            "31bcff9bc75c42e893b0299a32c27bb8",
            "a8ef1462d5d743b2be8658ee53951630"
          ]
        }
      },
      "source": [
        "cifar10_train = torchvision.datasets.CIFAR10(root='./cifar10', \n",
        "                                             train=True, \n",
        "                                             transform=None, \n",
        "                                             target_transform=None,\n",
        "                                             download=True)\n",
        "cifar10_test = torchvision.datasets.CIFAR10(root='./cifar10', \n",
        "                                             train=False, \n",
        "                                             transform=None, \n",
        "                                             target_transform=None,\n",
        "                                             download=True)\n",
        "\n",
        "# Divides the dataset into train and val so that we can use the val to choose our hyperparameters\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(cifar10_train, [40000, 10000], \n",
        "                                                           generator=torch.Generator().manual_seed(12345))\n",
        "test_dataset = cifar10_test"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./cifar10/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e86a1b7e1d7a45ecb133555f6d52e3e7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./cifar10/cifar-10-python.tar.gz to ./cifar10\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_NzhpK8KvQC"
      },
      "source": [
        "# Helper code to support adding different transforms on the dataset lazily after downloading the dataset\n",
        "# From https://discuss.pytorch.org/t/apply-different-transform-data-augmentation-to-train-and-validation/63580/5\n",
        "class MapDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Given a dataset, creates a dataset which applies a mapping function\n",
        "    to its items (lazily, only when an item is called).\n",
        "\n",
        "    Note that data is not cloned/copied from the initial dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dataset, map_fn):\n",
        "        self.dataset = dataset\n",
        "        self.map = map_fn\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.map:     \n",
        "            x = self.map(self.dataset[index][0]) \n",
        "        else:     \n",
        "            x = self.dataset[index][0]  \n",
        "        y = self.dataset[index][1]         \n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMbXzcUiKc7e"
      },
      "source": [
        "#### Standard transforms to apply on images - Convert to tensors and normalize with mean and std. These are the basic transforms that you will always apply. The mean and std have been pre calculated on the training set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWEKU9xQKbkW"
      },
      "source": [
        "# Notice that we apply the same mean and std normalization calculated on train, to both the train and test datasets.\n",
        "test_transform = transforms.Compose([\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(\n",
        "                                         [0.4373, 0.4434, 0.4725],\n",
        "                                         [0.1201, 0.1231, 0.1052])\n",
        "                                     ])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(\n",
        "                                          [0.4373, 0.4434, 0.4725],\n",
        "                                          [0.1201, 0.1231, 0.1052])\n",
        "                                      ])\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrDIxTs2JwbO"
      },
      "source": [
        "train_dataset_w_transform  = MapDataset(train_dataset, train_transform)\n",
        "val_dataset_w_transform = MapDataset(val_dataset, test_transform)\n",
        "test_dataset_w_transform = MapDataset(test_dataset, test_transform)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LcExDS1tyjI"
      },
      "source": [
        "bs = 128\n",
        "torch.backends.cudnn.benchmark = True\n",
        "train_loader = DataLoader(train_dataset_w_transform, batch_size=bs, shuffle=True, drop_last=False,num_workers=10, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset_w_transform, batch_size=bs, shuffle=False, drop_last=False,num_workers=10, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False, drop_last=False,num_workers=10, pin_memory=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXLFsEihoMTn"
      },
      "source": [
        "### Q 2.1 Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksECLwMqkgj_"
      },
      "source": [
        "In this question, fill in the missing parts to build a generic training loop that returns the train and validation losses and accuracies. The #TODOs will guide you through the key points and you should fill some code for each #TODO. You might need to add some additional code for bookkeeping the losses and accuracies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnmWg9P9N5U_"
      },
      "source": [
        "def train_loop(model, criterion, optimizer,  train_loader, val_loader):\n",
        "    \"\"\"\n",
        "    Generic training loop\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : Object instance of your model class \n",
        "    criterion : Loss function \n",
        "    optimizer : Instance of optimizer class of your choice \n",
        "    train_loader : Training data loader \n",
        "    val_loader : Validation data loader\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_losses : List with train loss on dataset per epoch\n",
        "    train_accuracies : List with train accuracy on dataset per epoch\n",
        "    val_losses : List with validation loss on dataset per epoch\n",
        "    val_accuracies : List with validation accuracy on dataset per epoch\n",
        "\n",
        "    \"\"\"\n",
        "    best_val = 0.0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    max_patience = 5\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training\n",
        "    for t in tqdm(range(50)):\n",
        "\n",
        "        # TODO : Set the model to train mode        \n",
        "        model.train()\n",
        "        # TODO: Loop over the training set \n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # TODO: Put the inputs and targets on the write device\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            # TODO: Feed forward to get the logits\n",
        "            output = model(data)\n",
        "            # TODO: Compute the loss and accuracy\n",
        "            loss = criterion(output, target)\n",
        "            correct = (target.eq(output.long())).sum()\n",
        "            accuracy = 100. * correct/len(train_loader)\n",
        "            # TODO: zero the gradients before running\n",
        "            optimizer.zero_grad()\n",
        "            # the backward pass.\n",
        "            \n",
        "            # TODO: Backward pass to compute the gradient\n",
        "            # of loss w.r.t our learnable params. \n",
        "            loss.backward()\n",
        "            # TODO: Update params\n",
        "            optimizer.step()\n",
        "            # TODO: Keep track of accuracy and loss\n",
        "            train_losses.append(loss)\n",
        "            train_accuracies.append(accuracy)\n",
        "        \n",
        "        # TODO: Switch the model to eval mode\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # TODO: Loop over the validation set \n",
        "            for batch_idx, (data, target) in enumerate(val_loader):\n",
        "                # TODO: Put the inputs and targets on the write device\n",
        "                data, target = data.to(device), target.to(device)\n",
        "                # TODO: Feed forward to get the logits\n",
        "                output = model(data)\n",
        "                # TODO: Compute the loss and accuracy\n",
        "                loss = criterion(output, target)\n",
        "                correct = (target.eq(output.long())).sum()\n",
        "                accuracy = 100. * correct/len(train_loader)\n",
        "                val_losses.append(loss)\n",
        "                val_accuracies.append(accuracy)\n",
        "                                                \n",
        "                # TODO: Keep track of accuracy and loss\n",
        "                correct = (target.eq(output.long())).sum()  \n",
        "        if val_accuracies[-1] > best_val:\n",
        "          best_val = val_accuracies[-1]\n",
        "          patience_counter = 0\n",
        "\n",
        "          # TODO: Save best model, optimizer, epoch_number\n",
        "          best_model = model\n",
        "          best_optimizer = optimizer\n",
        "          best_epoch_number = t\n",
        "        else:\n",
        "          patience_counter += 1    \n",
        "          if patience_counter > max_patience: \n",
        "            break\n",
        "\n",
        "        print(\"[EPOCH]: %i, [TRAIN LOSS]: %.6f, [TRAIN ACCURACY]: %.3f\" % (t, train_losses[-1], train_accuracies[-1]))\n",
        "        print(\"[EPOCH]: %i, [VAL LOSS]: %.6f, [VAL ACCURACY]: %.3f \\n\" % (t, val_losses[-1] ,val_accuracies[-1]))\n",
        "\n",
        "    return train_losses, train_accuracies, val_losses, val_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sdCTYUouY4o"
      },
      "source": [
        "### Q 2.2 Shallow convolutional network. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKqYI2qxp95J"
      },
      "source": [
        "class View(nn.Module):\n",
        "    def __init__(self, shape):\n",
        "      super().__init__()\n",
        "      self.shape = shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.view(*self.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDHE0lOgpF0W"
      },
      "source": [
        "\n",
        "ShallowNet =  nn.Sequential(\n",
        "      nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size=5, padding=2),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2),\n",
        "      nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=2),\n",
        "      nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size=3, padding=1),\n",
        "      nn.ReLU(),\n",
        "      nn.MaxPool2d(kernel_size=8),\n",
        "      View((-1,256)),\n",
        "      nn.Linear(256, 10),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV26KJLDrzhx"
      },
      "source": [
        "#### Write the object oriented version of ShallowNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8YygdvnuqDM"
      },
      "source": [
        "class ShallowConvnet(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        \"\"\"\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_channels : Number of input channels\n",
        "        num_classes : Number of classes for the final prediction \n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(in_channels = input_channels, out_channels = 64, kernel_size=5, padding=2),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels = 64, out_channels = 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels = 128, out_channels = 256, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=8),\n",
        "            View((-1,256)),\n",
        "            nn.Linear(256, num_classes),\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        output : Result after running through the model\n",
        "        \"\"\"\n",
        "        \n",
        "        # TODO\n",
        "        x = x.view(-1, self.input_size)\n",
        "        return self.network(x)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjEH5jIhwnet"
      },
      "source": [
        "### Q2.3 Instantiate the model and run this using an SGD optimizer, with the appropriate loss function for classification\n",
        "\n",
        "Report the learning curves (training and validation accuracy vs number of epochs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oG1E-YRu4Cj"
      },
      "source": [
        "# TODO : Initialize the model and cast to correct device\n",
        "\n",
        "# TODO : Initialize the criterion\n",
        "\n",
        "# TODO : Initialize the SGD optimizer with lr 1e-3\n",
        "\n",
        "# TODO : Run the training loop using this model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLRvwxlJoFQO"
      },
      "source": [
        "### Q2.4 Simple convolution network - "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSUz9ljRxRc5"
      },
      "source": [
        "Design a convolutional neural network with the following specification: \n",
        "For each convolution layer, use appropriate padding such that it maintains the resolution of the image. The resolution should be changing only when you introduce maxpooling layers. Each convolution layer should be followed by a relu non-linearity. The first two blocks containing 3 convolutional layers are each followed by a maxpooling layer that halves the resolution of the image. After the third block, use maxpooling to get a resolution of 1 X 1. Finally, apply a linear transformation to project to the number of classes. \n",
        "\n",
        "Structure of the convolution layers of the model:\n",
        "1. Number of input channels to the model = 3\n",
        "2. First convolution layer of kernel size 5 with filter size 64 and padding such that it maintains the resolution of the image.\n",
        "3. Followed by a block of 3 convolution layers of kernel size 3, filter size 64 and padding such that it maintains the resolution of the image.\n",
        "5. Followed by 3 convolution layers of kernel size 3, filter size 128 and padding such that it maintains the resolution of the image.\n",
        "6. Followed by 3 convolution layers of kernel size 3, filter size 256 and padding such that it maintains the resolution of the image. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfwKR3uNlsNc"
      },
      "source": [
        "# Use the description of the structure of the model and the hints given below \n",
        "\n",
        "class SimpleConvnet(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        super(SimpleConvnet, self).__init__()\n",
        "        # TODO\n",
        "\n",
        "    \n",
        "    def forward(self, x):\n",
        "       \n",
        "        # TODO \n",
        "\n",
        "        # HINT: first conv layer \n",
        "\n",
        "\n",
        "        # HINT: block of 3 conv \n",
        "\n",
        "\n",
        "        # HINT : block of 3 conv \n",
        "\n",
        "\n",
        "        # HINT: block of 3 conv \n",
        "\n",
        "\n",
        "        # HINT: projection \n",
        "\n",
        "        \n",
        "        return output\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_WKoU3ewH2B"
      },
      "source": [
        "# TODO : Initialize the model and cast to correct device\n",
        "\n",
        "# TODO : Initialize the criterion \n",
        "\n",
        "# TODO : Initialize the optimizer \n",
        "\n",
        "# TODO : Run the training loop using this model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZRJhVst0BLv"
      },
      "source": [
        "### Q 2.5 Report results of training using SGD optimizer for both ShallowNet and SimpleConvnet. What do you observe?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uv6ExxBvLL3Y"
      },
      "source": [
        "### Q 2.6 Add batch normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8IFrRqC0WwA"
      },
      "source": [
        "#### Q2.6 a After each relu layer, add a batch normalization layer to the network SimpleConvnet you created above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7nvtAeBoiUX"
      },
      "source": [
        "class SimpleConvnet2(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        super(SimpleConvnet2, self).__init__()\n",
        "        \n",
        "        # TODO\n",
        "    \n",
        "    def forward(self, x):\n",
        "       \n",
        "       # TODO\n",
        "        \n",
        "        return output\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgqQzKV10tz7"
      },
      "source": [
        "# TODO : Initialize the model and cast to correct device\n",
        "\n",
        "# TODO : Initialize the criterion \n",
        "\n",
        "# TODO : Initialize the optimizer \n",
        "\n",
        "# TODO : Run the training loop using this model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5wqxHGa0vZe"
      },
      "source": [
        "#### Q2.6 b Plot the the training curves (training loss vs \\# epochs, training accuracy vs # epochs) using SGD (lr 1e-3) with and without batch normalization. Comment on the difference. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZW0227y408ej"
      },
      "source": [
        "#### Q2.6 c. Try running the same two networks with an Adam optimizer (lr 1e-4). Plot the the training curves (training loss vs \\# epochs, training accuracy vs # epochs) with and without batch normalization. Comment on the difference. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeVzHHciAt-W"
      },
      "source": [
        "#### Q2.6 d Once you choose an optimizer and see that it does train, make sure your model has enough capacity by overfitting on one batch of the data set. You should be able to get 100% train accuracy. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANkTYeTWn6Mv"
      },
      "source": [
        "### Q 2.7 Add residual connections"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A55lJPET13Q7"
      },
      "source": [
        "#### Residual connections help stabilise training and lead to faster convergence. In this question you will introduce residual connections into the SimpleConvnet2 model that you built above. \n",
        "\n",
        "We will add residual connections after each block of 3 convolutional layers. Lets consider the first block of three convolutional layers. The input to this block, the so called residual, is added to the output of the block before the final batch normalization layer of that block. \n",
        "\n",
        "\n",
        "IMP NOTE: You will notice that the number of filters of these two summands are not the same. For this, you will need to use a convolution layer on the residual component, which changes the number of filters while keeping the rest of the dimensions the same. This can be achieved with a careful selection of the input_channels, output_channels, kernel_size and padding parameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzxWZNrmUzFT"
      },
      "source": [
        "class ResidualConvnet(nn.Module):\n",
        "    def __init__(self, input_channels, num_classes):\n",
        "        super(ResidualConvnet, self).__init__()\n",
        "        \n",
        "        # TODO\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # TODO \n",
        "        return output\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn9OWZiceenh"
      },
      "source": [
        "# TODO : Initialize the model and cast to correct device\n",
        "\n",
        "# TODO : Initialize the criterion \n",
        "\n",
        "# TODO : Initialize the optimizer \n",
        "\n",
        "# TODO : Run the training loop using this model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRrWJ63c4C0f"
      },
      "source": [
        "### Q 2.8 Plot the training curves with and without the residual connection. Comment on the difference. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QXPnxcrx-VU"
      },
      "source": [
        "### Q2.9 Reducing overfiting \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HISbvoBh8JTJ"
      },
      "source": [
        "In the previous questions, you might have observed that there is a large difference between the training and validation losses. This is a sign that the model is overfitting. One way to combat this is by adding random transformations to the input data to make your model more robust and prevent it from memorizing the input data. \n",
        "\n",
        "Torchvision provides several transforms that you can readily apply to your data. Experiment with adding a few transforms and report your results in terms of learning curves to see if the gap between the training and validation loss reduces and try to achieve better perfomance on the validation set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cZRQd-VK7ev"
      },
      "source": [
        "test_transform = transforms.Compose([\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(\n",
        "                                         [0.4373, 0.4434, 0.4725],\n",
        "                                         [0.1201, 0.1231, 0.1052])\n",
        "                                     ])\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "                                      # TODO: Add more transforms here \n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(\n",
        "                                          [0.4373, 0.4434, 0.4725],\n",
        "                                          [0.1201, 0.1231, 0.1052])\n",
        "                                      ])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brZrUl7mK7ez"
      },
      "source": [
        "train_dataset_w_transform  = MapDataset(train_dataset, train_transform)\n",
        "val_dataset_w_transform = MapDataset(val_dataset, test_transform)\n",
        "test_dataset_w_transform = MapDataset(test_dataset, test_transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZp6DqwfK7e1"
      },
      "source": [
        "bs = 128\n",
        "train_loader = DataLoader(train_dataset_w_transform, batch_size=bs, shuffle=True, drop_last=False)\n",
        "val_loader = DataLoader(val_dataset_w_transform, batch_size=bs, shuffle=False, drop_last=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False, drop_last=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR_9tihT5Vmu"
      },
      "source": [
        "#### Use the residual network that you built above and use data augmentation to reduce the overfitting "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxMLVdrUwh--"
      },
      "source": [
        "# TODO : Initialize the model and cast to correct device\n",
        "\n",
        "# TODO : Initialize the criterion \n",
        "\n",
        "# TODO : Initialize the optimizer \n",
        "\n",
        "# TODO : Run the training loop using this model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2rhPXb4nRFA"
      },
      "source": [
        "### Q 2.10  Effect of learning rate decay "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDqvgBeptLpD"
      },
      "source": [
        "#### Here you need to change the training loop to have one additional operation - add the scheduler step at the end of each epoch. Experiment with different learning rate schedulers provided by pytorch. Report results using atleast StepLR. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoDm84bvtbYK"
      },
      "source": [
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbGSCk4N57qX"
      },
      "source": [
        "def train_loop2(model, criterion, optimizer, scheduler,  train_loader, val_loader):\n",
        "    \"\"\"\n",
        "    Generic training loop\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : Object instance of your model class \n",
        "    criterion : Loss function \n",
        "    optimizer : Instance of optimizer class of your choice \n",
        "    scheduler : Instance of scheduler class of your choice \n",
        "    train_loader : Training data loader \n",
        "    val_loader : Validation data loader\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    train_losses : List with train loss on dataset per epoch\n",
        "    train_accuracies : List with train accuracy on dataset per epoch\n",
        "    val_losses : List with validation loss on dataset per epoch\n",
        "    val_accuracies : List with validation accuracy on dataset per epoch\n",
        "\n",
        "    \"\"\"\n",
        "    best_val = 0.0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "    max_patience = 5\n",
        "    patience_counter = 0\n",
        "\n",
        "    # Training\n",
        "    for t in tqdm(range(50)):\n",
        "\n",
        "        # TODO : Set the model to train mode        \n",
        "\n",
        "        # TODO: Loop over the training set \n",
        "\n",
        "            # TODO: Put the inputs and targets on the write device\n",
        "            \n",
        "            # TODO: Feed forward to get the logits\n",
        "\n",
        "            # TODO: Compute the loss and accuracy\n",
        "\n",
        "            # TODO: zero the gradients before running\n",
        "            # the backward pass.\n",
        "\n",
        "            # TODO: Backward pass to compute the gradient\n",
        "            # of loss w.r.t our learnable params. \n",
        "\n",
        "            # TODO: Update params\n",
        "            \n",
        "            # TODO: Keep track of accuracy and loss\n",
        "\n",
        "        \n",
        "        # Switch the model to eval mode\n",
        "        # TODO\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # TODO: Loop over the validation set \n",
        "            \n",
        "                # TODO: Put the inputs and targets on the write device\n",
        "            \n",
        "                # TODO: Feed forward to get the logits\n",
        "\n",
        "                # TODO: Compute the loss and accuracy\n",
        "\n",
        "                # TODO: Keep track of accuracy and loss\n",
        "\n",
        "        if val_accuracies[-1] > best_val:\n",
        "          best_val = val_accuracies[-1]\n",
        "          patience_counter = 0\n",
        "\n",
        "          # TODO: Save best model, optimizer, epoch_number\n",
        "          \n",
        "        else:\n",
        "          patience_counter += 1    \n",
        "          \n",
        "          if patience_counter > max_patience: \n",
        "            break\n",
        "\n",
        "        print(\"[EPOCH]: %i, [TRAIN LOSS]: %.6f, [TRAIN ACCURACY]: %.3f\" % (t, train_losses[-1], train_accuracies[-1]))\n",
        "        print(\"[EPOCH]: %i, [VAL LOSS]: %.6f, [VAL ACCURACY]: %.3f \\n\" % (t, val_losses[-1] ,val_accuracies[-1]))\n",
        "\n",
        "        # TODO : scheduler step\n",
        "\n",
        "    return train_losses, train_accuracies, val_losses, val_accuracies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bK1P1aU0sbJn"
      },
      "source": [
        "# TODO : Initialize the model and cast to correct device\n",
        "\n",
        "# TODO : Initialize the criterion \n",
        "\n",
        "# TODO : Initialize the optimizer \n",
        "\n",
        "# TODO : Initialize the \n",
        "\n",
        "# TODO : Run the training loop using this model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG8bhd5V97iJ"
      },
      "source": [
        "### Q2.11 Hyper parameter tuning \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G05PHYHJ-DF2"
      },
      "source": [
        "#### Experiment with a range of learning rates and optimizers, as well as the parameter in the learning rate scheduler for StepLR. Report the following plots: \n",
        "\n",
        "1. Learning curves (training and validation loss for 5 different learning rate with SGD optimizer)\n",
        "2. Learning curves (training and validation loss for 5 different learning rate with Adam optimizer)\n",
        "3. Learning curves (training and validation loss for 5 different gamma parameter for the StepLR)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiLexWP36qp6"
      },
      "source": [
        "### Q2.12 Load the model that gave you best validation accuracy and report results on the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz-m4xZU_nV8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}